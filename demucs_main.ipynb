{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demucs Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import numpy as np\n",
    "\n",
    "%config Completer.use_jedi = False\n",
    "\n",
    "# Unknownerror, cudnn 어쩌고저쩌고 에러\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from demucs_model.ipynb\n",
      "sample_inputs.shape: (None, 441000, 2)\n",
      "e1_output.shape: (None, 110250, 64)\n",
      "e2_output.shape: (None, 27563, 128)\n",
      "e3_output.shape: (None, 6891, 256)\n",
      "e4_output.shape: (None, 1723, 512)\n",
      "e5_output.shape: (None, 431, 1024)\n",
      "e6_output.shape: (None, 108, 2048)\n",
      "bi_outputs.shape: (None, 108, 4096)\n",
      "dense_outputs.shape: (None, 108, 2048)\n",
      "d6_outputs.shape: (None, 432, 1024)\n",
      "d5_outputs.shape: (None, 1724, 512)\n",
      "d4_outputs.shape: (None, 6892, 256)\n",
      "d3_outputs.shape: (None, 27564, 128)\n",
      "d2_outputs.shape: (None, 110252, 64)\n",
      "d1_outputs.shape: (None, 441000, 8)\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "from demucs_model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from demucs_CustomTraining.ipynb\n",
      "Number of files in train sets: 100\n",
      "Number of files in test sets: 50\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "from demucs_CustomTraining import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Phil\\venvs/demucs\n",
      "C:\\Users\\Phil\\venvs/demucs\\musdb18\n"
     ]
    }
   ],
   "source": [
    "base_dir = os.path.join(os.getcwd(), \"venvs/demucs\")\n",
    "data_base_dir = os.path.join(base_dir, \"musdb18\")\n",
    "print(base_dir)\n",
    "print(data_base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make directories to store stems\n",
    "os.makedirs(os.path.join(base_dir, \"musdb18_npz\", \"train\"))\n",
    "os.makedirs(os.path.join(base_dir, \"musdb18_npz\", \"test\"))\n",
    "os.makedirs(os.path.join(base_dir, \"reconstructed\"))\n",
    "os.makedirs(os.path.join(base_dir, \"checkpoints\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Classic Education - NightOwl  length:  171.24716553287982 seconds long, file size:  576.1723918914795 MB.\n",
      "ANiMAL - Clinic A  length:  237.86521541950114 seconds long, file size:  800.3130168914795 MB.\n",
      "ANiMAL - Easy Tiger  length:  205.47337868480724 seconds long, file size:  691.3286418914795 MB.\n",
      "ANiMAL - Rockshow  length:  165.51183673469387 seconds long, file size:  556.8755168914795 MB.\n",
      "Actions - Devil's Words  length:  196.6265759637188 seconds long, file size:  661.5630168914795 MB.\n",
      "Actions - One Minute Smile  length:  163.37560090702948 seconds long, file size:  549.6880168914795 MB.\n",
      "Actions - South Of The Water  length:  176.61097505668934 seconds long, file size:  594.2192668914795 MB.\n",
      "Aimee Norwich - Child  length:  189.08009070294784 seconds long, file size:  636.1723918914795 MB.\n",
      "Alexander Ross - Goodbye Bolero  length:  418.63256235827663 seconds long, file size:  1408.5161418914795 MB.\n",
      "Alexander Ross - Velvet Curtain  length:  514.2987755102041 seconds long, file size:  1730.3911418914795 MB.\n",
      "Angela Thomas Wade - Milk Cow Blues  length:  210.90684807256235 seconds long, file size:  709.6098918914795 MB.\n",
      "Atlantis Bound - It Was My Fault For Waiting  length:  268.051156462585 seconds long, file size:  901.8755168914795 MB.\n",
      "Auctioneer - Our Future Faces  length:  207.70249433106576 seconds long, file size:  698.8286418914795 MB.\n",
      "AvaLuna - Waterduct  length:  259.1114739229025 seconds long, file size:  871.7973918914795 MB.\n",
      "BigTroubles - Phantom  length:  146.7501133786848 seconds long, file size:  493.7505168914795 MB.\n",
      "Bill Chudziak - Children Of No-one  length:  230.73668934240362 seconds long, file size:  776.3286418914795 MB.\n",
      "Black Bloc - If You Want Success  length:  398.5473015873016 seconds long, file size:  1340.9380168914795 MB.\n",
      "Celestial Shore - Die For Us  length:  278.47691609977323 seconds long, file size:  936.9536418914795 MB.\n",
      "Chris Durban - Celebrate  length:  301.6039909297052 seconds long, file size:  1014.7661418914795 MB.\n",
      "Clara Berry And Wooldog - Air Traffic  length:  173.26730158730157 seconds long, file size:  582.9692668914795 MB.\n",
      "Clara Berry And Wooldog - Stella  length:  195.5584580498866 seconds long, file size:  657.9692668914795 MB.\n",
      "Clara Berry And Wooldog - Waltz For My Victims  length:  175.2409977324263 seconds long, file size:  589.6098918914795 MB.\n",
      "Cnoc An Tursa - Bannockburn  length:  294.52190476190475 seconds long, file size:  990.9380168914795 MB.\n",
      "Creepoid - OldTree  length:  302.0219501133787 seconds long, file size:  1016.1723918914795 MB.\n",
      "Dark Ride - Burning Bridges  length:  232.6639455782313 seconds long, file size:  782.8130168914795 MB.\n",
      "Dreamers Of The Ghetto - Heavy Love  length:  294.80054421768705 seconds long, file size:  991.8755168914795 MB.\n",
      "Drumtracks - Ghost Bitch  length:  356.91392290249433 seconds long, file size:  1200.8598918914795 MB.\n",
      "Faces On Film - Waiting For Ga  length:  257.4396371882086 seconds long, file size:  866.1723918914795 MB.\n",
      "Fergessen - Back From The Start  length:  168.5536507936508 seconds long, file size:  567.1098918914795 MB.\n",
      "Fergessen - Nos Palpitants  length:  198.22875283446712 seconds long, file size:  666.9536418914795 MB.\n",
      "Fergessen - The Wind  length:  191.82004535147394 seconds long, file size:  645.3911418914795 MB.\n",
      "Flags - 54  length:  315.1644444444444 seconds long, file size:  1060.3911418914795 MB.\n",
      "Giselle - Moss  length:  201.71174603174603 seconds long, file size:  678.6723918914795 MB.\n",
      "Grants - PunchDrunk  length:  204.40526077097505 seconds long, file size:  687.7348918914795 MB.\n",
      "Helado Negro - Mitad Del Mundo  length:  181.67292517006803 seconds long, file size:  611.2505168914795 MB.\n",
      "Hezekiah Jones - Borrowed Heart  length:  241.3946485260771 seconds long, file size:  812.1880168914795 MB.\n",
      "Hollow Ground - Left Blind  length:  159.1031292517007 seconds long, file size:  535.3130168914795 MB.\n",
      "Hop Along - Sister Cities  length:  283.2370068027211 seconds long, file size:  952.9692668914795 MB.\n",
      "Invisible Familiars - Disturbing Wildlife  length:  218.4997732426304 seconds long, file size:  735.1567668914795 MB.\n",
      "James May - All Souls Moon  length:  220.84498866213153 seconds long, file size:  743.0473918914795 MB.\n",
      "James May - Dont Let Go  length:  241.95192743764173 seconds long, file size:  814.0630168914795 MB.\n",
      "James May - If You Say  length:  258.3219954648526 seconds long, file size:  869.1411418914795 MB.\n",
      "James May - On The Line  length:  256.0928798185941 seconds long, file size:  861.6411418914795 MB.\n",
      "Jay Menon - Through My Eyes  length:  253.1671655328798 seconds long, file size:  851.7973918914795 MB.\n",
      "Johnny Lokke - Promises & Lies  length:  285.81442176870746 seconds long, file size:  961.6411418914795 MB.\n",
      "Johnny Lokke - Whisper To A Scream  length:  255.32662131519274 seconds long, file size:  859.0630168914795 MB.\n",
      "Jokers, Jacks & Kings - Sea Of Leaves  length:  191.47174603174602 seconds long, file size:  644.2192668914795 MB.\n",
      "Leaf - Come Around  length:  264.3824036281179 seconds long, file size:  889.5317668914795 MB.\n",
      "Leaf - Summerghost  length:  231.80480725623582 seconds long, file size:  779.9223918914795 MB.\n",
      "Leaf - Wicked  length:  190.6358276643991 seconds long, file size:  641.4067668914795 MB.\n",
      "Lushlife - Toynbee Suite  length:  628.3784126984127 seconds long, file size:  2114.2193393707275 MB.\n",
      "Matthew Entwistle - Dont You Ever  length:  113.82421768707484 seconds long, file size:  382.9692668914795 MB.\n",
      "Meaxic - Take A Step  length:  282.5171882086168 seconds long, file size:  950.5473918914795 MB.\n",
      "Meaxic - You Listen  length:  412.5257142857143 seconds long, file size:  1387.9692668914795 MB.\n",
      "Music Delta - 80s Rock  length:  36.73396825396826 seconds long, file size:  123.59426689147949 MB.\n",
      "Music Delta - Beatles  length:  36.17668934240363 seconds long, file size:  121.71926689147949 MB.\n",
      "Music Delta - Britpop  length:  36.594648526077094 seconds long, file size:  123.12551689147949 MB.\n",
      "Music Delta - Country1  length:  34.5512925170068 seconds long, file size:  116.25051689147949 MB.\n",
      "Music Delta - Country2  length:  17.2756462585034 seconds long, file size:  58.12551689147949 MB.\n",
      "Music Delta - Disco  length:  124.59827664399093 seconds long, file size:  419.2192668914795 MB.\n",
      "Music Delta - Gospel  length:  75.55773242630386 seconds long, file size:  254.2192668914795 MB.\n",
      "Music Delta - Grunge  length:  41.65659863945578 seconds long, file size:  140.1567668914795 MB.\n",
      "Music Delta - Hendrix  length:  19.644081632653062 seconds long, file size:  66.09426689147949 MB.\n",
      "Music Delta - Punk  length:  28.583764172335602 seconds long, file size:  96.17239189147949 MB.\n",
      "Music Delta - Reggae  length:  17.2756462585034 seconds long, file size:  58.12551689147949 MB.\n",
      "Music Delta - Rock  length:  12.910294784580499 seconds long, file size:  43.43801689147949 MB.\n",
      "Music Delta - Rockabilly  length:  25.75092970521542 seconds long, file size:  86.64114189147949 MB.\n",
      "Night Panther - Fire  length:  212.8108843537415 seconds long, file size:  716.0161418914795 MB.\n",
      "North To Alaska - All The Same  length:  247.96589569160997 seconds long, file size:  834.2973918914795 MB.\n",
      "Patrick Talbot - A Reason To Leave  length:  259.5526530612245 seconds long, file size:  873.2817668914795 MB.\n",
      "Patrick Talbot - Set Me Free  length:  289.78503401360547 seconds long, file size:  975.0005168914795 MB.\n",
      "Phre The Eon - Everybody's Falling Apart  length:  224.23510204081632 seconds long, file size:  754.4536418914795 MB.\n",
      "Port St Willow - Stay Even  length:  316.8362811791383 seconds long, file size:  1066.0161418914795 MB.\n",
      "Remember December - C U Next Time  length:  242.53242630385486 seconds long, file size:  816.0161418914795 MB.\n",
      "Secret Mountains - High Horse  length:  355.31174603174605 seconds long, file size:  1195.4692668914795 MB.\n",
      "Skelpolu - Human Mistakes  length:  324.4988662131519 seconds long, file size:  1091.7973918914795 MB.\n",
      "Skelpolu - Together Alone  length:  325.8224036281179 seconds long, file size:  1096.2505168914795 MB.\n",
      "Snowmine - Curfews  length:  275.01714285714286 seconds long, file size:  925.3130168914795 MB.\n",
      "Spike Mullings - Mike's Sulking  length:  256.6965986394558 seconds long, file size:  863.6723918914795 MB.\n",
      "St Vitus - Word Gets Around  length:  247.0138775510204 seconds long, file size:  831.0942668914795 MB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steven Clark - Bounty  length:  289.2741950113379 seconds long, file size:  973.2817668914795 MB.\n",
      "Strand Of Oaks - Spacestation  length:  243.67020408163265 seconds long, file size:  819.8442668914795 MB.\n",
      "Sweet Lights - You Let Me Down  length:  391.7902947845805 seconds long, file size:  1318.2036418914795 MB.\n",
      "Swinging Steaks - Lost My Way  length:  309.9631746031746 seconds long, file size:  1042.8911418914795 MB.\n",
      "The Districts - Vermont  length:  227.97351473922902 seconds long, file size:  767.0317668914795 MB.\n",
      "The Long Wait - Back Home To Blue  length:  260.458231292517 seconds long, file size:  876.3286418914795 MB.\n",
      "The Scarlet Brand - Les Fleurs Du Mal  length:  303.4383673469388 seconds long, file size:  1020.9380168914795 MB.\n",
      "The So So Glos - Emergency  length:  166.81215419501135 seconds long, file size:  561.2505168914795 MB.\n",
      "The Wrong'Uns - Rothko  length:  202.15292517006802 seconds long, file size:  680.1567668914795 MB.\n",
      "Tim Taler - Stalker  length:  237.63301587301586 seconds long, file size:  799.5317668914795 MB.\n",
      "Titanium - Haunted Age  length:  248.10521541950112 seconds long, file size:  834.7661418914795 MB.\n",
      "Traffic Experiment - Once More (With Feeling)  length:  435.0722902494331 seconds long, file size:  1463.8286418914795 MB.\n",
      "Traffic Experiment - Sirens  length:  421.2796371882086 seconds long, file size:  1417.4223918914795 MB.\n",
      "Triviul - Angelsaint  length:  236.70421768707482 seconds long, file size:  796.4067668914795 MB.\n",
      "Triviul - Dorothy  length:  187.36181405895692 seconds long, file size:  630.3911418914795 MB.\n",
      "Voelund - Comfort Lives In Belief  length:  209.90839002267575 seconds long, file size:  706.2505168914795 MB.\n",
      "Wall Of Death - Femme  length:  238.93333333333334 seconds long, file size:  803.9067668914795 MB.\n",
      "Young Griffo - Blood To Bone  length:  254.3978231292517 seconds long, file size:  855.9380168914795 MB.\n",
      "Young Griffo - Facade  length:  167.857052154195 seconds long, file size:  564.7661418914795 MB.\n",
      "Young Griffo - Pennies  length:  277.803537414966 seconds long, file size:  934.6880168914795 MB.\n",
      "Wall time: 11min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for file in mus_train:\n",
    "    input_mixture = file.stems[0]\n",
    "    target_mixtures = file.stems[1:]\n",
    "    path = os.path.join(base_dir, \"musdb18_npz\", \"train\", file.name) + '.npz'\n",
    "    np.savez(path, input_mixture=input_mixture, target_mixtures=target_mixtures)\n",
    "    \n",
    "    length = file.stems[0].shape[0] / file.rate\n",
    "    file_size = os.path.getsize(path) / 1024 / 1024\n",
    "    print(file.name, ' length: ', length, 'seconds long, file size: ', file_size, \"MB.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AM Contra - Heart Peripheral, length: 209.91seconds long, file size: 706.25MB.\n",
      "Al James - Schoolboy Facination, length: 200.34seconds long, file size: 674.06MB.\n",
      "Angels In Amplifiers - I'm Alright, length: 179.40seconds long, file size: 603.59MB.\n",
      "Arise - Run Run Run, length: 219.20seconds long, file size: 737.50MB.\n",
      "BKS - Bulldozer, length: 336.85seconds long, file size: 1133.36MB.\n",
      "BKS - Too Much, length: 219.94seconds long, file size: 740.00MB.\n",
      "Ben Carrigan - We'll Talk About It All Tonight, length: 254.72seconds long, file size: 857.03MB.\n",
      "Bobby Nobody - Stitch Up, length: 221.05seconds long, file size: 743.75MB.\n",
      "Buitraker - Revo X, length: 275.64seconds long, file size: 927.42MB.\n",
      "Carlos Gonzalez - A Place For Us, length: 250.03seconds long, file size: 841.25MB.\n",
      "Cristina Vane - So Easy, length: 253.96seconds long, file size: 854.45MB.\n",
      "Detsky Sad - Walkie Talkie, length: 190.29seconds long, file size: 640.23MB.\n",
      "Enda Reilly - Cur An Long Ag Seol, length: 186.80seconds long, file size: 628.52MB.\n",
      "Forkupines - Semantics, length: 273.39seconds long, file size: 919.84MB.\n",
      "Georgia Wonder - Siren, length: 430.20seconds long, file size: 1447.42MB.\n",
      "Girls Under Glass - We Feel Alright, length: 317.09seconds long, file size: 1066.88MB.\n",
      "Hollow Ground - Ill Fate, length: 141.48seconds long, file size: 476.02MB.\n",
      "James Elder & Mark M Thompson - The English Actor, length: 205.29seconds long, file size: 690.70MB.\n",
      "Juliet's Rescue - Heartbeats, length: 267.47seconds long, file size: 899.92MB.\n",
      "Little Chicago's Finest - My Own, length: 281.43seconds long, file size: 946.88MB.\n",
      "Louis Cressy Band - Good Time, length: 292.64seconds long, file size: 984.61MB.\n",
      "Lyndsey Ollard - Catching Up, length: 227.23seconds long, file size: 764.53MB.\n",
      "M.E.R.C. Music - Knockout, length: 269.82seconds long, file size: 907.81MB.\n",
      "Moosmusic - Big Dummy Shake, length: 198.67seconds long, file size: 668.44MB.\n",
      "Motor Tapes - Shore, length: 246.53seconds long, file size: 829.45MB.\n",
      "Mu - Too Bright, length: 271.98seconds long, file size: 915.08MB.\n",
      "Nerve 9 - Pray For The Rain, length: 343.52seconds long, file size: 1155.78MB.\n",
      "PR - Happy Daze, length: 162.52seconds long, file size: 546.80MB.\n",
      "PR - Oh No, length: 76.00seconds long, file size: 255.70MB.\n",
      "Punkdisco - Oral Hygiene, length: 221.40seconds long, file size: 744.92MB.\n",
      "Raft Monk - Tiring, length: 212.07seconds long, file size: 713.52MB.\n",
      "Sambasevam Shanmugam - Kaathaadi, length: 317.70seconds long, file size: 1068.91MB.\n",
      "Secretariat - Borderline, length: 286.65seconds long, file size: 964.45MB.\n",
      "Secretariat - Over The Top, length: 250.91seconds long, file size: 844.22MB.\n",
      "Side Effects Project - Sing With Me, length: 243.48seconds long, file size: 819.22MB.\n",
      "Signe Jakobsen - What Have You Done To Me, length: 177.21seconds long, file size: 596.25MB.\n",
      "Skelpolu - Resurrection, length: 395.48seconds long, file size: 1330.63MB.\n",
      "Speak Softly - Broken Man, length: 252.98seconds long, file size: 851.17MB.\n",
      "Speak Softly - Like Horses, length: 312.47seconds long, file size: 1051.33MB.\n",
      "The Doppler Shift - Atrophy, length: 331.26seconds long, file size: 1114.53MB.\n",
      "The Easton Ellises (Baumi) - SDRNR, length: 234.50seconds long, file size: 788.98MB.\n",
      "The Easton Ellises - Falcon 69, length: 246.80seconds long, file size: 830.39MB.\n",
      "The Long Wait - Dark Horses, length: 305.53seconds long, file size: 1027.97MB.\n",
      "The Mountaineering Club - Mallory, length: 244.27seconds long, file size: 821.88MB.\n",
      "The Sunshine Garcia Band - For I Am The Moon, length: 320.11seconds long, file size: 1077.03MB.\n",
      "Timboz - Pony, length: 252.87seconds long, file size: 850.78MB.\n",
      "Tom McKenzie - Directions, length: 175.66seconds long, file size: 591.02MB.\n",
      "Triviul feat. The Fiend - Widow, length: 234.92seconds long, file size: 790.39MB.\n",
      "We Fell From The Sky - Not You, length: 207.70seconds long, file size: 698.83MB.\n",
      "Zeno - Signs, length: 234.20seconds long, file size: 787.97MB.\n",
      "Wall time: 6min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for file in mus_test:\n",
    "    input_mixture = file.stems[0]\n",
    "    target_mixtures = file.stems[1:]\n",
    "    path = os.path.join(base_dir, \"musdb18_npz\", \"test\", file.name) + '.npz'\n",
    "    np.savez(path, input_mixture=input_mixture, target_mixtures=target_mixtures)\n",
    "    \n",
    "    length = file.stems[0].shape[0] / file.rate\n",
    "    file_size = os.path.getsize(path) / 1024 / 1024\n",
    "    print(file.name, ', length: {:.2f}'.format(length), ' seconds long, file size: {:.2f}'.format(file_size), \"MB.\", sep='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random shift를 어떻게 구현해야하나...\n",
    "# 11초짜리 lenght를 뽑고, 10개의 random shift를 한 다음, 이것의 average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = mus_test[2]\n",
    "test_input_mixture = test_file.stems[0]\n",
    "\n",
    "optimizer = keras.optimizers.Adam(0.001)\n",
    "loss_fn = keras.losses.MeanAbsoluteError()\n",
    "\n",
    "EPOCHS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(input_batch, target_batch):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predicted_batch = demucs(input_batch, training=True)\n",
    "        loss = loss_fn(target_batch, predicted_batch)\n",
    "    trainable_vars = demucs.trainable_variables\n",
    "    gradients = tape.gradient(loss, trainable_vars)\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_ds(path, batch_size=BATCH_SIZE,\n",
    "           length=44100*11, long=44100*10, strides=44100):\n",
    "    loaded = np.load(path)\n",
    "    input_mixture = loaded['input_mixture']\n",
    "    target_mixtures = loaded['target_mixtures']\n",
    "    target_mixtures = np.column_stack(target_mixtures)\n",
    "    \n",
    "    ds_inp = tf.data.Dataset.from_tensor_slices((input_mixture))\n",
    "#     ds_inp = ds_inp.window(length, shift=strides, drop_remainder=True)\n",
    "    ds_inp = ds_inp.window(long, shift=strides, drop_remainder=True)\n",
    "    ds_inp = ds_inp.flat_map(lambda windows: windows.batch(length))\n",
    "    ds_inp = ds_inp.map(lambda windows: windows, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "    ds_tar = tf.data.Dataset.from_tensor_slices((target_mixtures))\n",
    "#     ds_tar = ds_tar.window(length, shift=strides, drop_remainder=True)\n",
    "    ds_tar = ds_tar.window(long, shift=strides, drop_remainder=True)\n",
    "    ds_tar = ds_tar.flat_map(lambda windows: windows.batch(length))\n",
    "    ds_tar = ds_tar.map(lambda windows: windows, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    \n",
    "    ds_total = [ds_inp, ds_tar]\n",
    "    total_ds = tf.data.Dataset.zip(tuple(ds_total))\n",
    "    total_ds = total_ds.batch(batch_size)\n",
    "    \n",
    "    return total_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recon_fn(epoch=1, test_input_mixture=test_input_mixture, long=44100 * 10):\n",
    "    ds_inp = tf.data.Dataset.from_tensor_slices((test_input_mixture))\n",
    "    ds_inp = ds_inp.window(long, shift=long, drop_remainder=True)\n",
    "    ds_inp = ds_inp.flat_map(lambda windows: windows.batch(long))\n",
    "    ds_inp = ds_inp.map(lambda windows: windows, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "    predicted = tf.zeros((1, long, 8), dtype=tf.float32)\n",
    "    for test_input_batch in ds_inp:\n",
    "        test_input_batch = tf.expand_dims(test_input_batch, 0)\n",
    "        pred_batch = demucs(test_input_batch)\n",
    "        predicted = tf.concat((predicted, pred_batch), axis=1)\n",
    "    predicted = tf.squeeze(predicted)\n",
    "    predicted = predicted[long:, :] # discard the first zeros\n",
    "    splited = tf.split(predicted, 4, axis=-1)\n",
    "    recon = tf.stack(splited)\n",
    "    \n",
    "    recon_dir = os.path.join(base_dir, \"reconstructed\")\n",
    "    for i in range(4):\n",
    "        to_write = tf.audio.encode_wav(recon[i], sample_rate=44100)\n",
    "        if i == 0:\n",
    "            file_name = os.path.join(recon_dir, '{}_drums.wav'.format(epoch))\n",
    "        elif i == 1:\n",
    "            file_name = os.path.join(recon_dir, '{}_bass.wav'.format(epoch))\n",
    "        elif i == 2:\n",
    "            file_name = os.path.join(recon_dir, '{}_other.wav'.format(epoch))\n",
    "        elif i == 3:\n",
    "            file_name = os.path.join(recon_dir, '{}_vocals.wav'.format(epoch))\n",
    "        tf.io.write_file(file_name, to_write)\n",
    "        print('Wrote', file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote C:\\Users\\Phil\\venvs/demucs\\reconstructed\\1_drums.wav\n",
      "Wrote C:\\Users\\Phil\\venvs/demucs\\reconstructed\\1_bass.wav\n",
      "Wrote C:\\Users\\Phil\\venvs/demucs\\reconstructed\\1_other.wav\n",
      "Wrote C:\\Users\\Phil\\venvs/demucs\\reconstructed\\1_vocals.wav\n",
      "Wall time: 11.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "recon_fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Phil\\\\venvs/demucs\\\\checkpoints\\\\cp-{epoch:03d}.ckpt'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_path = \"cp-{epoch:03d}.ckpt\"\n",
    "saving_path = os.path.join(base_dir, \"checkpoints\", checkpoint_path)\n",
    "saving_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_callback = keras.callbacks.ModelCheckpoint(filepath=saving_path,\n",
    "                                                verbose=1,\n",
    "                                                save_weights_only=True,\n",
    "                                                save_freq='epoch')\n",
    "callbacks = keras.callbacks.CallbackList([model_callback], model=demucs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch num 1\n",
      "....................................................................................................Time taken: 22057.25 seconds\n",
      "Start of epoch num 2\n",
      "....................................................................................................Time taken: 22134.74 seconds\n",
      "Start of epoch num 3\n",
      "....................................................................................................Time taken: 22350.55 seconds\n",
      "Start of epoch num 4\n",
      "....................................................................................................Time taken: 22295.62 seconds\n",
      "Start of epoch num 5\n",
      "....................................................................................................Time taken: 22298.22 seconds\n",
      "Wrote C:\\Users\\Phil\\venvs/demucs\\reconstructed\\5_drums.wav\n",
      "Wrote C:\\Users\\Phil\\venvs/demucs\\reconstructed\\5_bass.wav\n",
      "Wrote C:\\Users\\Phil\\venvs/demucs\\reconstructed\\5_other.wav\n",
      "Wrote C:\\Users\\Phil\\venvs/demucs\\reconstructed\\5_vocals.wav\n",
      "\n",
      "Epoch 00005: saving model to C:\\Users\\Phil\\venvs/demucs\\checkpoints\\cp-005.ckpt\n",
      "Start of epoch num 6\n",
      "....................................................................................................Time taken: 22431.28 seconds\n",
      "Start of epoch num 7\n",
      "....................................................................................................Time taken: 22528.44 seconds\n",
      "Start of epoch num 8\n",
      ".................................................."
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    print('Start of epoch num {}'.format(epoch + 1))\n",
    "    \n",
    "    epoch_time = time.time()\n",
    "    for file in mus_train:  \n",
    "        npz_path = os.path.join(base_dir, \"musdb18_npz\", \"train\", file.name) + '.npz'\n",
    "        loaded = np.load(npz_path)\n",
    "        input_mixture = loaded['input_mixture']\n",
    "        target_mixtures = loaded['target_mixtures']\n",
    "        \n",
    "        train_ds = gen_ds(npz_path)\n",
    "        train_ds = train_ds.prefetch(tf.data.AUTOTUNE)\n",
    "        \n",
    "        for i, (input_batch, target_batch) in enumerate(train_ds):\n",
    "            loss = train_step(input_batch, target_batch)ㅍ\n",
    "        print('.', end='')\n",
    "    print('Time taken: {:.2f} seconds'.format(time.time()-epoch_time))\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        recon_fn(epoch+1)\n",
    "        callbacks.on_epoch_end(epoch, logs=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_ds(path, batch_size=BATCH_SIZE,\n",
    "           length=44100*11, long=44100*10, strides=44100, random_time=random.randint(0, 44100)):\n",
    "    loaded = np.load(path)\n",
    "    input_mixture = loaded['input_mixture']\n",
    "    target_mixtures = loaded['target_mixtures']\n",
    "    target_mixtures = np.column_stack(target_mixtures)\n",
    "    \n",
    "    ds_inp = tf.data.Dataset.from_tensor_slices((input_mixture))\n",
    "    ds_inp = ds_inp.window(length, shift=strides, drop_remainder=True)\n",
    "    ds_inp = ds_inp.flat_map(lambda windows: windows.batch(length))\n",
    "    ds_inp = ds_inp.map(lambda windows: (windows[random_time:(random_time+long), :]),\n",
    "                       num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "    ds_tar = tf.data.Dataset.from_tensor_slices((target_mixtures))\n",
    "    ds_tar = ds_tar.window(length, shift=strides, drop_remainder=True)\n",
    "    ds_tar = ds_tar.flat_map(lambda windows: windows.batch(length))\n",
    "    ds_tar = ds_tar.map(lambda windows: (windows[random_time:(random_time+long), :]),\n",
    "                       num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    \n",
    "    ds_total = [ds_inp, ds_tar]\n",
    "    total_ds = tf.data.Dataset.zip(tuple(ds_total))\n",
    "    total_ds = total_ds.batch(batch_size)\n",
    "    \n",
    "    return total_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to verify that I'm doing is correct\n",
    "\n",
    "target_mixtures = np.column_stack(target_mixtures)\n",
    "ds_inp = tf.data.Dataset.from_tensor_slices((target_mixtures))\n",
    "ds_inp = ds_inp.window(long, shift=long, drop_remainder=True)\n",
    "ds_inp = ds_inp.flat_map(lambda windows: windows.batch(long))\n",
    "ds_inp = ds_inp.map(lambda windows: windows, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "predicted = tf.zeros((1, long, 8), dtype=tf.float32)\n",
    "for test_input_batch in ds_inp:\n",
    "    test_input_batch = tf.expand_dims(test_input_batch, 0)\n",
    "    pred_batch = tf.cast(test_input_batch, dtype=tf.float32)\n",
    "    predicted = tf.concat((predicted, pred_batch), axis=1)\n",
    "predicted = tf.squeeze(predicted)\n",
    "predicted = predicted[long:, :] # discard the first zeros\n",
    "splited = tf.split(predicted, 4, axis=-1)\n",
    "recon = tf.stack(splited)\n",
    "\n",
    "for i in range(4):\n",
    "    to_write = tf.audio.encode_wav(recon[i], sample_rate=44100)\n",
    "    if i == 0:\n",
    "        file_name = 'drums.wav'\n",
    "    elif i == 1:\n",
    "        file_name = 'bass.wav'\n",
    "    elif i == 2:\n",
    "        file_name = 'other.wav'\n",
    "    elif i == 3:\n",
    "        file_name = 'vocals.wav'\n",
    "    tf.io.write_file(file_name, to_write)\n",
    "    print('Wrote', file_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demucs",
   "language": "python",
   "name": "demucs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
